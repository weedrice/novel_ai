# LLM Server

AI ê¸°ë°˜ ì†Œì„¤/ëŒ€ë³¸ ì‘ì„±ì„ ìœ„í•œ LLM ì„œë²„ì…ë‹ˆë‹¤. ìºë¦­í„° ëŒ€ì‚¬ ìƒì„±, ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„, ì—í”¼ì†Œë“œ ìš”ì•½ ë“±ì˜ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‹ ì£¼ìš” ê¸°ëŠ¥

- **ëŒ€ì‚¬ ìƒì„± (Dialogue Generation)**: ìºë¦­í„° í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€ì‚¬ ì œì•ˆ
- **ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± (Scenario Generation)**: ë‹¤ì¤‘ ìºë¦­í„° ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
- **ëŒ€ë³¸ ë¶„ì„ (Script Analysis)**: ìºë¦­í„°, ëŒ€ì‚¬, ì¥ë©´, ê´€ê³„ ì¶”ì¶œ
- **ì—í”¼ì†Œë“œ ë¶„ì„ (Episode Analysis)**: ìš”ì•½, ìºë¦­í„° ë¶„ì„, ì¥ë©´ ë¶„ì„, ë§ì¶¤ë²• ê²€ì‚¬
- **ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (SSE Streaming)**: Server-Sent Events ê¸°ë°˜ ì‹¤ì‹œê°„ ëŒ€ì‚¬ ìƒì„±
- **ë©€í‹° LLM í”„ë¡œë°”ì´ë”**: OpenAI GPT, Anthropic Claude, Google Gemini ì§€ì›

## ğŸ›  ê¸°ìˆ  ìŠ¤íƒ

- **Framework**: FastAPI 0.115.0+
- **Language**: Python 3.11+
- **LLM Providers**:
  - OpenAI (GPT-3.5-turbo)
  - Anthropic Claude (claude-3-haiku-20240307)
  - Google Gemini (gemini-pro)
- **Architecture**: Controller-Service Pattern
- **Test Coverage**: 89% (109 tests)
- **Type Safety**: Pydantic 2.10.0+

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒ í™˜ê²½ ìƒì„± ë° í™œì„±í™”
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
cp .env.example .env
# .env íŒŒì¼ì„ ì—´ì–´ API í‚¤ ì…ë ¥
```

### 2. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

`.env` íŒŒì¼ì— ë‹¤ìŒ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:

```bash
# LLM Provider ì„ íƒ (openai, claude, gemini ì¤‘ íƒ 1)
DEFAULT_LLM_PROVIDER=openai

# OpenAI ì„¤ì •
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.8
OPENAI_MAX_TOKENS=150

# Anthropic Claude ì„¤ì •
ANTHROPIC_API_KEY=your-anthropic-api-key-here
ANTHROPIC_MODEL=claude-3-haiku-20240307
ANTHROPIC_TEMPERATURE=0.8
ANTHROPIC_MAX_TOKENS=150

# Google Gemini ì„¤ì •
GOOGLE_API_KEY=your-google-api-key-here
GEMINI_MODEL=gemini-pro
GEMINI_TEMPERATURE=0.8
GEMINI_MAX_TOKENS=150
```

### 3. ì„œë²„ ì‹¤í–‰

```bash
# ê°œë°œ ëª¨ë“œ (ìë™ ì¬ì‹œì‘)
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000

# í”„ë¡œë•ì…˜ ëª¨ë“œ
python app/main.py
```

ì„œë²„ê°€ ì‹¤í–‰ë˜ë©´ ë‹¤ìŒ ì£¼ì†Œì—ì„œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤:
- API ì„œë²„: http://localhost:8000
- API ë¬¸ì„œ (Swagger UI): http://localhost:8000/docs
- API ë¬¸ì„œ (ReDoc): http://localhost:8000/redoc

## ğŸ“¡ API ì—”ë“œí¬ì¸íŠ¸

### ì‹œìŠ¤í…œ ì—”ë“œí¬ì¸íŠ¸

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/` | ì„œë²„ ì •ë³´ ë° ë²„ì „ |
| GET | `/health` | í—¬ìŠ¤ ì²´í¬ |
| GET | `/providers` | ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë” ëª©ë¡ |

### ëŒ€ì‚¬ ìƒì„± ì—”ë“œí¬ì¸íŠ¸

#### POST `/gen/suggest`
ìºë¦­í„° í˜ë¥´ì†Œë‚˜ ê¸°ë°˜ ëŒ€ì‚¬ ì œì•ˆ

**Request Body:**
```json
{
  "character_name": "ê¹€ë¯¼ìˆ˜",
  "personality": "ë°ê³  ê¸ì •ì ì¸ ì„±ê²©",
  "speaking_style": "ì¹œê·¼í•˜ê³  ê²©ì‹ ì—†ëŠ” ë§íˆ¬",
  "context": "ì¹œêµ¬ì™€ ì¹´í˜ì—ì„œ ì»¤í”¼ë¥¼ ë§ˆì‹œë©° ëŒ€í™”",
  "honorific": "í•´ìš”ì²´"
}
```

**Response:**
```json
{
  "suggestions": [
    {
      "dialogue": "ì˜¤ëŠ˜ ë‚ ì”¨ ì •ë§ ì¢‹ì£ ?",
      "reason": "ë°ì€ ì„±ê²©ê³¼ ê¸ì •ì ì¸ ì–´ì¡°ê°€ ë“œëŸ¬ë‚¨",
      "score": 0.95
    }
  ]
}
```

#### POST `/gen/suggest-stream`
SSE ê¸°ë°˜ ì‹¤ì‹œê°„ ëŒ€ì‚¬ ìƒì„± (ë™ì¼í•œ Request Body)

### ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì—”ë“œí¬ì¸íŠ¸

#### POST `/gen/scenario`
ë‹¤ì¤‘ ìºë¦­í„° ëŒ€í™” ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±

**Request Body:**
```json
{
  "scenario_description": "ì»¤í”¼ìˆì—ì„œ ìš°ì—°íˆ ë§Œë‚œ ì˜› ì¹œêµ¬ë“¤ì˜ ì¬íšŒ",
  "characters": [
    {
      "name": "ê¹€ë¯¼ìˆ˜",
      "personality": "ë°ê³  ê¸ì •ì ",
      "speaking_style": "ì¹œê·¼í•œ ë§íˆ¬"
    }
  ],
  "tone": "ë”°ëœ»í•˜ê³  ê°ë™ì ì¸",
  "num_dialogues": 5
}
```

### ëŒ€ë³¸ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸

#### POST `/gen/analyze-script`
ëŒ€ë³¸ì—ì„œ ìºë¦­í„°, ëŒ€ì‚¬, ì¥ë©´, ê´€ê³„ ì¶”ì¶œ

**Request Body:**
```json
{
  "script_text": "ê¹€ë¯¼ìˆ˜: ì˜¤ëœë§Œì´ì•¼!\nì´ì˜í¬: ì •ë§ ì˜¤ëœë§Œì´ë„¤!"
}
```

**Response:**
```json
{
  "characters": [...],
  "dialogues": [...],
  "scenes": [...],
  "relationships": [...]
}
```

### ì—í”¼ì†Œë“œ ë¶„ì„ ì—”ë“œí¬ì¸íŠ¸

| Method | Endpoint | Description |
|--------|----------|-------------|
| POST | `/gen/episode/summary` | ì—í”¼ì†Œë“œ ìš”ì•½ ìƒì„± |
| POST | `/gen/episode/characters` | ì—í”¼ì†Œë“œ ë‚´ ìºë¦­í„° ë¶„ì„ |
| POST | `/gen/episode/scenes` | ì¥ë©´ ë¶„ì„ |
| POST | `/gen/episode/dialogues` | ëŒ€ì‚¬ ë¶„ì„ |
| POST | `/gen/episode/spell-check` | ë§ì¶¤ë²• ê²€ì‚¬ |

**Request Body (ê³µí†µ):**
```json
{
  "title": "ì—í”¼ì†Œë“œ ì œëª©",
  "content": "ì—í”¼ì†Œë“œ ë‚´ìš©..."
}
```

## ğŸ— ì•„í‚¤í…ì²˜

í”„ë¡œì íŠ¸ëŠ” **Controller-Service íŒ¨í„´**ì„ ë”°ë¦…ë‹ˆë‹¤:

```
llm-server/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                    # FastAPI ì•± ì§„ì…ì , ì˜ì¡´ì„± ì£¼ì…
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ llm_provider_manager.py  # LLM í”„ë¡œë°”ì´ë” ê´€ë¦¬
â”‚   â”œâ”€â”€ controllers/               # HTTP ìš”ì²­ ì²˜ë¦¬, ë¼ìš°íŒ…
â”‚   â”‚   â”œâ”€â”€ system_controller.py
â”‚   â”‚   â”œâ”€â”€ dialogue_controller.py
â”‚   â”‚   â”œâ”€â”€ scenario_controller.py
â”‚   â”‚   â”œâ”€â”€ script_analysis_controller.py
â”‚   â”‚   â”œâ”€â”€ episode_analysis_controller.py
â”‚   â”‚   â””â”€â”€ streaming_controller.py
â”‚   â”œâ”€â”€ services/                  # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§, LLM í˜¸ì¶œ
â”‚   â”‚   â”œâ”€â”€ dialogue_service.py
â”‚   â”‚   â”œâ”€â”€ scenario_service.py
â”‚   â”‚   â”œâ”€â”€ script_analysis_service.py
â”‚   â”‚   â””â”€â”€ episode_analysis_service.py
â”‚   â”œâ”€â”€ models/                    # Pydantic ìš”ì²­/ì‘ë‹µ ëª¨ë¸
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/                     # ìœ í‹¸ë¦¬í‹° (JSON íŒŒì‹±, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿)
â”‚       â”œâ”€â”€ json_parser.py
â”‚       â”œâ”€â”€ prompt_builder.py
â”‚       â””â”€â”€ prompt_templates.py
â”œâ”€â”€ tests/                         # í…ŒìŠ¤íŠ¸ (89% ì»¤ë²„ë¦¬ì§€)
â”œâ”€â”€ Dockerfile                     # Docker ì´ë¯¸ì§€ ë¹Œë“œ
â”œâ”€â”€ requirements.txt               # Python ì˜ì¡´ì„±
â””â”€â”€ .env.example                   # í™˜ê²½ ë³€ìˆ˜ ì˜ˆì‹œ
```

### ì£¼ìš” ì„¤ê³„ ì›ì¹™

1. **ê´€ì‹¬ì‚¬ì˜ ë¶„ë¦¬ (Separation of Concerns)**
   - Controller: HTTP ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬
   - Service: ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë° LLM í˜¸ì¶œ
   - Model: ë°ì´í„° ê²€ì¦ ë° íƒ€ì… ì•ˆì „ì„±

2. **ì˜ì¡´ì„± ì£¼ì… (Dependency Injection)**
   - `main.py`ì—ì„œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ì»¨íŠ¸ë¡¤ëŸ¬ì— ì£¼ì…
   - í…ŒìŠ¤íŠ¸ ìš©ì´ì„± ë° ìœ ì—°ì„± í–¥ìƒ

3. **ì—ëŸ¬ ì²˜ë¦¬ ë° Fallback**
   - ëª¨ë“  ì„œë¹„ìŠ¤ ë©”ì„œë“œì—ì„œ ì˜ˆì™¸ ì²˜ë¦¬
   - LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜

4. **ë©€í‹° í”„ë¡œë°”ì´ë” ì§€ì›**
   - `LLMProviderManager`ë¥¼ í†µí•œ í†µí•© ì¸í„°í˜ì´ìŠ¤
   - í™˜ê²½ ë³€ìˆ˜ë¡œ í”„ë¡œë°”ì´ë” ì„ íƒ

## ğŸ§ª í…ŒìŠ¤íŠ¸

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
# ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
pytest

# ì»¤ë²„ë¦¬ì§€ í¬í•¨ í…ŒìŠ¤íŠ¸
pytest --cov=app --cov-report=html

# íŠ¹ì • í…ŒìŠ¤íŠ¸ íŒŒì¼ ì‹¤í–‰
pytest tests/test_dialogue_service.py
```

### í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€

í˜„ì¬ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€: **89%** (109ê°œ í…ŒìŠ¤íŠ¸ í†µê³¼)

| ëª¨ë“ˆ | ì»¤ë²„ë¦¬ì§€ |
|------|---------|
| Controllers | 79-100% |
| Services | 66-94% |
| Models | 100% |
| Utils | 84-100% |
| Core | 79% |

ìƒì„¸ ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸: [coverage_report.md](./coverage_report.md)

## ğŸ³ Docker ë°°í¬

### ì´ë¯¸ì§€ ë¹Œë“œ ë° ì‹¤í–‰

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t llm-server:latest .

# ì»¨í…Œì´ë„ˆ ì‹¤í–‰
docker run -d \
  -p 8000:8000 \
  --env-file .env \
  --name llm-server \
  llm-server:latest

# ë¡œê·¸ í™•ì¸
docker logs -f llm-server
```

## ğŸ”§ ê°œë°œ ê°€ì´ë“œ

### ìƒˆë¡œìš´ ì—”ë“œí¬ì¸íŠ¸ ì¶”ê°€

1. **Pydantic ëª¨ë¸ ì •ì˜** (`app/models/`)
   ```python
   class MyRequest(BaseModel):
       field: str
   ```

2. **ì„œë¹„ìŠ¤ ë¡œì§ êµ¬í˜„** (`app/services/`)
   ```python
   class MyService:
       def __init__(self, llm_manager: LLMProviderManager):
           self.llm_manager = llm_manager

       def process(self, request: MyRequest) -> MyResponse:
           # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
           pass
   ```

3. **ì»¨íŠ¸ë¡¤ëŸ¬ ìƒì„±** (`app/controllers/`)
   ```python
   def create_my_router(service: MyService) -> APIRouter:
       router = APIRouter()

       @router.post("/my-endpoint")
       def my_endpoint(request: MyRequest):
           return service.process(request)

       return router
   ```

4. **ë¼ìš°í„° ë“±ë¡** (`app/main.py`)
   ```python
   my_service = MyService(llm_manager)
   my_router = create_my_router(my_service)
   app.include_router(my_router)
   ```

### í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ê´€ë¦¬

í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ `app/utils/prompt_templates.py`ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤:

```python
DIALOGUE_SUGGESTION_TEMPLATE = """
ë‹¹ì‹ ì€ ì†Œì„¤ ì‘ê°€ì˜ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ìºë¦­í„°: {character_name}
ì„±ê²©: {personality}
...
"""
```

### ì—ëŸ¬ í•¸ë“¤ë§

ëª¨ë“  ì„œë¹„ìŠ¤ ë©”ì„œë“œëŠ” try-exceptë¡œ ì—ëŸ¬ë¥¼ ì²˜ë¦¬í•˜ê³  fallbackì„ ì œê³µí•©ë‹ˆë‹¤:

```python
try:
    # LLM í˜¸ì¶œ
    result = llm_manager.generate(...)
except Exception as e:
    logger.error(f"Error: {e}")
    # Fallback ì‘ë‹µ ë°˜í™˜
    return default_response
```

## ğŸ“Š ë¡œê¹…

ë¡œê¹… ë ˆë²¨: `INFO`

ë¡œê·¸ í˜•ì‹:
```
2024-01-15 10:30:45 - app.services.dialogue_service - INFO - Generating dialogues...
```

ì£¼ìš” ë¡œê·¸ ì´ë²¤íŠ¸:
- LLM í”„ë¡œë°”ì´ë” ì´ˆê¸°í™”
- API í˜¸ì¶œ ì‹œì‘/ì™„ë£Œ
- ì—ëŸ¬ ë°œìƒ ì‹œ ìƒì„¸ ì •ë³´

## ğŸ¤ ê¸°ì—¬ ê°€ì´ë“œ

1. ë¸Œëœì¹˜ ìƒì„±: `git checkout -b feature/your-feature`
2. ì½”ë“œ ì‘ì„± ë° í…ŒìŠ¤íŠ¸ ì¶”ê°€
3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `pytest --cov=app`
4. ì»¤ë°‹: `git commit -m "feat: add your feature"`
5. Push: `git push origin feature/your-feature`
6. Pull Request ìƒì„±

## ğŸ“ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [FastAPI ê³µì‹ ë¬¸ì„œ](https://fastapi.tiangolo.com/)
- [OpenAI API ë¬¸ì„œ](https://platform.openai.com/docs)
- [Anthropic Claude API ë¬¸ì„œ](https://docs.anthropic.com/)
- [Google Gemini API ë¬¸ì„œ](https://ai.google.dev/)
